\section{Evaluation}
\label{sec:evaluation}

To evaluate the efficacy of our approach, we formulate three research questions:

\begin{enumerate}[label=RQ\arabic*]
	\item \label[requirement]{resqu1} What is the quality and quantity of the proposed methods for dependency collection?
	\item \label[requirement]{resqu2} What is the quality and quantity of the proposed method for mining usage samples?
	\item \label[requirement]{resqu3} How well is the proposed tool applicable?
\end{enumerate}

In the following, we will investigate each question.

\subsection{\ref{resqu1}: Dependency collection}
\label{sec:evaluation/resqu1}

To assess the quality and quantity of dependency collection, we analyze it with respect to extent, precision, recall, and performance.
\Cref{tab:evaluation/resqu1/quantities} shows the number of dependencies that we collected to build an experimental dataset from each data source following the proposed methods for a set of arbitrarily selected npm packages.
For each package, we have annotated a subset of the collected dependencies (max. 20 dependencies per package) to identify and classify false positive hits.

\begin{table}
	\caption{Quantity and false-positive rates (FPR) of downstream dependencies found by both methods for selected packages.}
	\label{tab:evaluation/resqu1/quantities}

	\centering
	\small
	\input{sections/5_evaluation/resqu1/quantities}
\end{table}

\paragraph{Extent}
\label{sec:evaluation/resqu1/extent}

Regarding the total number of dependencies collected by each method, no clear trend in favor of any method can be ascertained for small or medium packages (having less than \num{10,000} stars on GitHub).
For larger packages (having at least \num{10,000} stars), however, the number of dependencies collected from the npm registry stagnates near to 400 hits; beyond this limit, the npm registry will return internal server errors only.
For Sourcegraph, we have not hit any limitations so far; currently, they do not provide any official documentation for the exact rate limits.

As the disjunct proportion of results from both methods is very small (on average about \SI{6}{\percent}), we consider a combination of both data sources useful for maximizing the extent and diversity of the gained dataset.

\paragraph{Precision}
\label{sec:evaluation/resqu1/precision}

The precision of collected dependencies is drastically lower for dependencies found on Sourcegraph than for such found on npm.
\Cref{fig:evaluation/resqu1/fp_causes} collates the causes we have identified for false positives; most frequently, repositories specify a dependency on a target package in their package manifest file but do not import this package at any place.

\begin{figure}
	\centering
	\small
	% Fix font size of \thead
	\renewcommand\theadfont{\small}
	\input{sections/5_evaluation/resqu1/fp_causes}

	\caption{Causes for false-positive dependency matches and their frequencies.}
	\label{fig:evaluation/resqu1/fp_causes}
\end{figure}

In some situations, this may happen if the package is a plugin for another package or if it is invoked as a CLI from a build script, but in the majority of repositories, developers trivially appear to have specified the upstream dependency by accident (e.g., while copying a \code{package.json} file over from another package), or to have forgotten to remove the upstream dependency after switching away from using the package.
Rarer causes include repositories that only add type definitions for TypeScript to a package but do not actually import it, as well as, especially for repositories found on Sourcegraph, other packages required by a repository that declares a peer dependency on the target package which needs to be fulfilled by the depending repository.

We explain the increased false-positive rate for Sourcegraph dependencies by our observation that packages found on npm typically stand out by their higher cohesion and professional maintenance, whereas GitHub projects are more likely run as imperfect hobby projects.

We stress that a reduced precision does not impair the quality of results displayed to package developers as all dependencies not containing at least one
usage sample can be easily filtered out, but downloading and analyzing any irrelevant packages lowers the performance of the approach.

\paragraph{Recall}
\label{sec:evaluation/resqu1/recall}

Besides the precision of collected dependencies, their recall is also of interest.
As the dependencies are collected in excerpts from two very large datasets, a quantitative analysis of false-negative hits based on manual annotation of these datasets would be too expensive for this paper.
Nevertheless, some causes will prevent a dependency from being found by our methods:

\begin{itemize}
	\item Packages without a proper dependency manifest cannot be detected by our methods which are based on exactly these metadata.
	\item On npm, only published packages will be found, creating a bias towards generic and professional software.
	\item On both platforms, results will be sorted based on intransparent criteria (which, as we speculate, include the number of direct dependents on npm and the recent update frequency on Sourcegraph, resp.).
		As we only fetch the first dependencies from both data sources, this is a likely source of further biases.
		These biases could be fought by always fetching all dependencies, but this would drastically reduce performance.
\end{itemize}

\paragraph{Performance}
\label{sec:evaluation/resqu1/performance}

\begin{table}
	\caption{Key performance metrics for both dependency collection methods.}
	\label{tab:evaluation/resqu1/performance}

	\centering
	\begin{threeparttable}
		% Fix font size of \thead
		\renewcommand\theadfont{\normalsize}
		\input{sections/5_evaluation/resqu1/performance}
		\begin{tablenotes}
			\footnotesize
			\item[\alphtnotetext{1}] Hardware specifications of the test machine: 7 vCPUs Intel Xeon Cascade Lake @ \SI{2.80}{\giga\hertz}, internet down speed ~\SI{1.8}{\giga\bit/\second}.
			\item[\alphtnotetext{2}] Effective speed while downloading multiple packages in parallel.
		\end{tablenotes}
	\end{threeparttable}
\end{table}

\Cref{tab:evaluation/resqu1/performance} gives some basic metrics comparing the performance of both data sources.
While searching downstream dependencies is slower by average on npm as we use a web scraper instead of an API for this source, npm packages are usually smaller as opposed to many monorepos on OSS platforms that contain multiple node.js packages, and they are faster to download since the download-git-repo package that we use for repositories found on Sourcegraph, as of today, only uses zipballs instead of tarballs while the latter would offer a higher compression rate.

\subsection{\ref{resqu2}: Usage mining}
\label{sec:evaluation/resqu2}

To assess the quality and quantity of usage mining, we examine the precision, recall, and performance of this processing step.
Due to the complexity of a detailed annotation process, we only consider the existence of found usage samples on the dependency level.
The absolute quantity of found usage samples largely varies for different packages and dependencies based on the semantic extent of the package and the coupling between package and dependency.

\paragraph{Precision}
\label{sec:evaluation/resqu2/precision}

Under laboratory conditions, false-positive usage samples cannot be emitted by our method if we assume the correctness of the TypeScript compiler.
There are only two theoretical exceptions to this invariant:
\begin{enumerate*}[label=(\roman*)]
	\item In case of a name collision between two packages, invalid or false positives usage samples may be emitted.
		However, the npm infrastructure attempts to rule out these collisions by using the package name as a unique ID for every published package.
	\item If dependency developers deliberately interfere with the TypeScript compiler (for instance, by suppressing type errors using a \verb|@ts-ignore| comment\footnote{\url{https://www.typescriptlang.org/docs/handbook/release-notes/typescript-2-6.html\#suppress-errors-in-ts-files-using--ts-ignore-comments}}), both false positive and false negative matches are possible.
\end{enumerate*}

Additionally, some npm packages contain minified sources only that are still searchable valid code but can provide significantly reduced readability.

\paragraph{Recall}
\label{sec:evaluation/resqu2/recall}

To evaluate the recall of our method, we have refined the experimental dataset collected in \cref{sec:evaluation/resqu1} and removed all false-positive dependencies.
Within this cleansed dataset, the overall rate of dependencies for that our method is unable to find at least one usage sample accounts for \SI{47.9}{\percent}.
To explain these false negatives, we have identified a number of systematic causes:

\begin{itemize}
	\item Many projects on OSS platforms have complex build configurations that include additional transpilation or code generation steps before a final version of the source code is reached that is valid to the node.js interpreter or the TypeScript compiler.
		Unless we add explicit support for such build configurations, type analysis and usage mining for these projects will fail.
	\item In some situations, additional type definitions are required to perform a complete type inference of a dependency.
		This applies to every parametrized callback from a package for that type definitions are not available.
		This limitation could be resolved by downloading or generating type definitions for all upstream dependencies of every downstream dependency; this, however, would reduce the performance of the approach.
	\item In general, the static type analyzer of TypeScript has its limitations.
		For instance, the type analysis will have a limited recall for certain control flow patterns or metaprogramming constructs such as meta-circular evaluation using the \code{eval()} function, dynamic function binding using \code{Function.bind()}, etc.
\end{itemize}

\paragraph{Performance}
\label{sec:evaluation/resqu2/performance}

The total time spent performing type analysis for a repository from the experimental dataset and mining usage samples averages ca. \num{3} seconds.
The complete ASF of an average parsed repository consumes between \SI{10}{\mega\byte} and \SI{500}{\mega\byte} RAM, depending on the size of the repository.

\subsection{\ref{resqu3}: Tool}
\label{sec:evaluation/resqu3}

To assess the applicability of our proposed tool, we investigate the different options of users who would like to answer the questions raised in \cref{sec:approach}.
We further analyze how far our tool meets the requirements posed above.

\paragraph{Answering User Questions}

To answer \cref{q1} without our tool, users could access the aforementioned data sources manually to search for and view downstream dependencies.
The source code of repositories can be browsed on Sourcegraph, but if an npm package is not available on an OSS platform, users will need to download and extract it.

With our tool, the data collection process is condensed into a single button that displays all references from both data sources incrementally.
Users can hover or click any dependency to view its documentation or implementation.

To answer \cref{q2,q3} without our tool, users need to scan each found dependency separately to count or read all usage samples.
If they are intending to analyze the usage of a member with an unambiguous name, they can perform an expeditious string search.
This is possible using Sourcegraph or another search engine if the dependency is indexed there; otherwise, users will need to download the repository and search it locally.
If the member of interest has an ambiguous name, users will need to download the project and view it in an IDE such as VS Code that supports reference search.
Alternatively, they can try the precise code intelligence mode on Sourcegraph (see \cref{sec:related_work/usage_samples}); however, this feature is available for few repositories only.

Within our tool, all usage samples can be collected automatically and be merged into a single list.
Once the collected dependencies have been processed, users can select a member of interest and view all its usage samples with a single click.
For the usage analysis of Java APIs, the tool \emph{Exapus} \citep{de2013multi} also considers different kinds of member usages such as instantiation vs. inheritance of a class; a similar classification could also improve the usability of our tool.

\paragraph{Meeting Requirements}

We break down \cref{req1} into three acceptance criteria: application liveness, a small resource footprint, and an easy setup.

As learned from the previous research questions, our tool is able to process 5 up to 12 dependencies per minute while requiring about \SI{500}{\mega\byte} RAM in total and less than \SI{30}{\mega\byte} ROM per package.
After the tool is activated, the first results usually take less than 10 seconds to appear on the UI.
This is an acceptable delay regarding Shneiderman's definition of acceptable application response times \citep{shneiderman2010designing}, so the liveness criterion is fulfilled.
Even if the tool downloads a few hundred dependencies, it will occupy less than \SI{10}{\giga\byte} ROM that can be released at any time, which we consider a small resource footprint at a time where modern operating systems require \SI{64}{\giga\byte} ROM; thus, also the second acceptance criterion is fulfilled.
As our extension can be installed with two clicks from the Visual Studio Code Extension Marketplace, it also fulfills the setup criterion and thus the first requirement.

\Cref{req2} effectively calls for a minimum number of context switches that are required to use the tool, i.e., for minimizing the temporal, spatial, and semantic distance \citep{ungar1997debugging} between the downstream dependency UI and the IDE used by package developers.

Once the data are available in the UI, interaction with them is possible with the same speed as with local source files in the package.
This meets Shneiderman's requirement to frequent tasks and indicates a low temporal distance.

The spatial distance varies for the different views of the tool:
For the dependency browser and the usage browser, a separate toolbar has to be opened, which indicates a higher spatial distance; still, the artifacts are available in the same IDE.
Also, the dependency artifacts have no strong relationship to any existing IDE artifacts, so any closer integration into an existing view would increase coupling between independent domains.
The code annotations are displayed close to the package source code with a small spatial distance.

The semantic distance can be described as the perceived similarity of artifacts displayed both in the tool's UI and in the existing IDE, i.e., the package and dependency members.
Most items in the usage browser bear the same label as the corresponding identifier in the source code, which reduces the semantic distance.
However, dependency member path nodes that refer to anonymous source code expressions are displayed differently than in the outline view of VS Code (see \cref{fig:implementation/presentation/screenshot/dependency_browser}), increasing the semantic distance a bit.

With a low temporal distance, a low-to-medium spatial distance, and a mainly low semantic distance, our tool also fulfills \cref{req2}.
