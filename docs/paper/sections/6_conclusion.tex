\section{Conclusion and Future Work}
\label{sec:conclusion}

Downstream dependencies provide a promising perspective for package developers that are interested in understanding how their interfaces are being used in practice by other software developers.
In this paper, we have proposed a novel approach to making this sort of information accessible to package developers by automating the dependency collection and mining of usage samples from every dependency.
We have identified two kinds of rich data sources for collecting downstream dependencies efficiently: public package repositories, such as npm, and online code search engines for finding dependent repositories by their package manifest files, namely Sourcegraph.
We accomplish the usage mining by scanning the ASTs of every dependency for certain usage patterns that refer to a type from the target package; if the dependency uses a dynamically typed language, we perform a type analysis before.
We have demonstrated the fundamental usability of our approach with a tool that embeds the downstream dependency data into the VS Code IDE.

Our analysis suggests that the collected dependencies and the extracted usage samples have a viable quality in terms of precision and extent and that the resource requirements of our implementation are sufficiently low for collecting all data on the local machine of a package developer.
Nevertheless, we have identified a number of causes for false negatives over the whole mining process that are predestined to biasing the output usage samples, including the popularity and recency of dependencies but also the proper declaration of packages and their renunciation of complex toolchains and metaprogramming patterns.
Our analysis, however, is not yet fully supported by data and would require further quantitative evaluation to assess the exact precision and recall of our approach for mining usage samples as well as the efficacy of our tool.
As for the first question, manual annotation of source code repositories with usage samples will be required to compare them to the outputs of our implementation.
Close insights for the second question could be gained from a user study that compares the efficiency of package developers asked to solve a set of downstream dependency-related tasks with or without the help of our tool.

We believe that our tool has further potential to support package developers in surveying downstream dependencies and thus have assembled a shortlist of open to-dos for the project.
Among others, we want to expand the applicability of the tool by adding support for further ecosystems and programming languages.
The user experience of the tool could be improved by increasing the liveness of displayed data, for instance, by automatically reflecting source code changes.
For better performance, usage samples could be cached, and finer-grained UI updates could be implemented.
Last but not least, we want to improve the integration of the extension by adding support for browser environments such as github.dev\footnote{\url{https://code.visualstudio.com/api/extension-guides/web-extensions}}\footnoteseparator\footnote{\url{https://docs.github.com/en/codespaces/the-githubdev-web-based-editor}} and by connecting it to the GitHub Remote Repositories extension\footnote{\url{https://marketplace.visualstudio.com/items?itemName=GitHub.remotehub}}.

In a wider perspective, we envision many other purposes for analyzing the collected downstream dependency data.
By deriving usage patterns from the samples using an approach such as \cite{zhong2009mapo}, the structure and classification of raw data could be refined.
The derived patterns could also be exploited as suggestions for convenience protocols to package developers.
There are many ways to turn dependency data into metrics that either could be used for classifying downstream dependencies (e.g., according to their coupling to the target package as per LCOM \citep{chidamber1994metrics}), or that could be attributed to ranges of packages for comparing them concerning their dependency-based cohesion or their spreading in an ecosystem.

By replacing the static usage analysis with a dynamic approach, the detected usage samples could be enriched with valuable runtime data describing involved parameters, the fine-grained code coverage of package members, or the invocation context of members.

Returning to the abstract problem that we have formulated in the introduction -- how can we improve developer knowledge about their packages' usage? --, further data sources next to source code files emerge that could be mined for package references, too.
For instance, we are looking forward to mining the change history of projects for package-related changes (potentially indicating breaking changes), searching conversation platforms such as public issue trackers or Q\&A forums for mentions of package members and analyzing their sentiments (potentially indicating confusing behavior or bad documentation), or even scanning continuous integration logs for failure stack traces (potentially revealing bugs in the package of interest).
