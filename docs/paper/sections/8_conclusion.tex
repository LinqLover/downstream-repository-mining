\section{Conclusion and future work}
\label{sec:conclusion}

Downstream dependencies provide a promising perspective for package developers that are interested in understanding how their interfaces are being used in the wild by other software developers.
In this paper, we have proposed a novel approach to making this sort of information accessible to package developers by automating the dependency collection and mining of usage samples from every dependency.
We have identified two kinds of rich data sources for collecting downstream dependencies efficiently: public package repositories, such as npm, and online code search engines for finding dependent repositories by their package manifest files, namely Sourcegraph.
We accomplish the usage mining by scanning the ASTs of every dependency for certain usage patterns that refer to a type from the target package; if the dependency uses a dynamically typed language, we perform a type analysis beforehand.
We have demonstrated the fundamental usability of our approach with a tool that embeds the downstream dependency data into the Visual Studio Code IDE.

Our analysis suggests that the collected dependencies and the extracted usage samples have a viable quality in terms of precision and extent, and that the resource requirements of our implementation are sufficiently low for collecting all data on the local machine of a package developer.
However, we have identified a number of causes for false negatives over the whole mining process that are predestined to biasing the output usage samples, including the popularity and recency of dependencies but also the proper declaration of packages and their renunciation of complex toolchains and metaprogramming patterns.
Our analysis, however, is not yet fully supported by data and would require further quantitative evaluation to assess the exact precision and recall of our usage samples mining approach as well as the efficacy of our tool.
To answer the first question, source code repositories will need to be annotated manually with all usage samples to compare these annotations to the outputs of our implementation.
To answer the second question, the best strategy would be to run a user study that compares the efficiency of package developers asked to solve a set of downstream dependency-related tasks with or without the help of our tool.

We believe that our tool has further potential to support package developers in surveying downstream dependencies and thus have assembled a shortlist of open to-dos for the project:

\begin{description}
	\item[Expand applicability of the tool:]
		Add support for further ecosystems and programming languages (e.g., JavaScript packages distributed over content delivery networks and imported from HTML; the Python/pip environment; the .NET ecosystem).
		Our implementation already provides key abstractions that will facilitate the integration of other data sources.
	\item[Optimize the dataflow:]
		Store mined usage samples in a file system cache to reuse them in a later IDE session.
		Reduce the number of UI updates to improve the performance and operability of the tool.
	\item[Increase liveness of displayed data:]
		Automatically reflect source code changes in the displayed package structure.
		In JavaScript projects with separate typings, mirror the code annotations from the type definition files into the implementation files.
	\item[Improve the integration of the extension:]
		Convert the extension into a web extension to support browser environments such as github.dev.\footnote{\url{https://code.visualstudio.com/api/extension-guides/web-extensions}}\footnote{\url{https://docs.github.com/en/codespaces/the-githubdev-web-based-editor}}
		Integrate the GitHub Remote Repositories extension for VS Code\footnote{\url{https://marketplace.visualstudio.com/items?itemName=GitHub.remotehub}} to make browsing dependencies more convenient.
\end{description}

In a wider perspective, we envision many other purposes for analyzing the collected downstream dependency data.
By deriving usage patterns from the samples using an approach such as MAPO \cite{zhong2009mapo}, the structure and classification of raw data could be refined.
The derived patterns could also be exploited as suggestions for convenience protocols to package developers.
There are many ways to turn dependency data into metrics that either could be used for classifying downstream dependencies (for instance, according to their coupling to the target package (LCOM)), or that could be attributed to ranges of packages for comparing them with regard to their dependency-based cohesion or their spreading in an ecosystem.

By replacing the static usage analysis with a dynamic approach, the detect usage samples could be enriched with valuable runtime data describing involved parameters, the fine-grained code coverage of package members, or the invocation context of members.
Returning to the abstract problem that we have formulated in the introduction -- how can we improve developer knowledge about their packages' usage? --, further data sources next to source code files emerge that could be mined for package references, too.
For instance, we are looking forward to mining the change history of projects for package-related changes (potentially indicating breaking changes), searching conversation platforms such as public issue trackers or Q\&A forums for mentions of package members and analyzing their sentiments (potentially indicating confusing behavior or bad documentation), or even scanning continuous integration logs for failure stack traces (potentially revealing bugs in the package of interest).
